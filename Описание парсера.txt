Первым делом подгружаются библиотеки для работы кода. 
Selenium WebDriver – программная библиотека для управления браузерами. Существуют такие виды парсинга, как эмулятор браузера и запросы к серверу. Так как этот вид поддерживает Java Script, он и будет использован в моей работе.  
Pandas - Программная библиотека на языке Python для обработки и анализа данных. В моей работе будет использована для вывода полученных данных в файл csv.
BeautifulSoup - библиотека Python для парсинга HTML и XML документов. Часто используется для скрапинга веб-страниц.
Библиотека requests является стандартным инструментом для составления HTTP-запросов в Python. 
Также используется библиотека time для установки времени простоя, либо же загрузки страницы.

Tripadvisor. 
Для начала мы вписываем путь до исполняемого файла, то есть до браузера на компьютере и записываем все это в driver. Далее вводим ссылку сайта и получаем с помощью driver.get. После ввода ссылки необходимого сайта, создаем глобальный массив для сбора всех последующих массивов, указываем в отдельном параметре текущую страницу, с которой браузер будет начинать работу, в моем случае это первая страница. 
Далее происходит поиск всех карточек на странице. Для каждого параметра ищем элемент внутри карточки с помощью find_element, а именно name-название ресторана, t_and_price – тип и средняя цена, small_text – маленький текст в карточке, review_num – полученные из маленького текста количество отзывов, rating –рейтинг с указанием вида вывода данных через точку, reviews -  отзывы. В конце цикла все полученные данные записываются в один массив, который далее добавляется в глобальный массив.
Вместе с получением данных из карточек проводится проверка на вариант, что в карточке нет информации по цене, по рейтингу или по обоим параметрам сразу. Также происходит проверка на ошибку, что если программа выдает ошибку, то не останавливается, а просто переходит к следующей карточке.
Далее прописываем следующую логику, что если страница не прогружается, 3 раза пробуем нажать на переход к новой странице. Следующим шагом идет еще один способ получения элемента в html с помощью xpath с той страницы, на которой мы находимся в текущий момент, при возникновении ошибки. Если не получается перейти 3 раза на следующую страницу – цикл заканчивается.
Далее задаем функцию sleep на 2 секунды, чтобы не перегружать сайт и для того, чтобы страница полностью прогрузилась. Далее идет проверка на страницу, на которой мы находимся. На сайте Tripadvisor следующая логика кнопок: с 1 по 4 страницу мы видим первые четыре кнопки, далее, начиная с пятой, первая кнопка пропадает и появляется шестая и так далее. Поэтому, проверяя данное условие мы добавляем следующую страницу для сохранения порядка. И под конец с помощью pandas создаем таблицу с наименованием столбцов и выводим файл в формате csv.
После работы парсера, получаем файл csv.

Яндекс Карты.
Как и в прошлом источнике, сначала подключаем необходимые библиотеки и указываем ссылку сайта. Указываем параметр времени, который будет означать с какой задержкой будет прогружаться сайт, в моем случае это 0,1 секунда. Задаем параметр а, который проверяет сколько прошло времени на данный момент для дальнейшего шага. Начинаем цикл, в котором прокручиваются все объявления ресторанов на странице и проверяем сколько прошло времени. Если данная итерация заняла более 200 секунд с задержкой в 0,1 секунду, даем 5 секунд отдыха.
Проделываем те же шаги, что и с сайтов Tripadvisor, просматриваем все карточки и вытягиваем конкретные поля, после чего добавляем эти поля в массив. Было замечено, что количество отзывов имеет разный вариант написания, поэтому проверяем каждый из случаев, во втором – убираем слева и справа знак скобочки, и так же записываем в нужный параметр. После этого добавляем в глобальный массив и выводим в удобном формате наш файл.

Leclick.ru.
Данный сайт имеет следующую схему: на странице выводится по 12 карточек, чтобы получить следующие нужно в адресной строке прописывать новое значение.  Поэтому задаем параметр offset и приравниваем изначально к нулю. Снова создаем глобальный массив, прописываем url адрес сайта и обозначаем, что offset будет меняться каждый раз на плюс 12. Используем BeautifulSoup и разделяем все данные по карточкам. Далее вытягиваем наши параметры и добавляем в массив, который потом добавляем в глобальный массив. Задаем параметр сна на 2 секунды и выгружаем получившийся файл.
